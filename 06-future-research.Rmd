---
bibliography: bibliography.bib
csl: ieee-with-url.csl
---

```{r setup6, include=FALSE}
library(data.table)
library(dplyr)
library(ggplot2)
library(knitr)
library(multiwayvcov)
library(stargazer)
opts_chunk$set(echo=FALSE, message=FALSE, warning = FALSE)
stargazer.type <- opts_knit$get('rmarkdown.pandoc.to')
stargazer.type <- ifelse(is.null(stargazer.type), 'text', ifelse('latex' == stargazer.type, 'latex', 'html'))
```

# Applications and Future Research

## Solidifying the Result

```{r covariate_relabel6}
df.complete <- read.csv('data/Merged Data.csv')

df.newheader <- c(
  'id', 'mind','energy', 'nature', 'tactics', 'identity', 'experimenter',
  'in.treatment', 'gender', 'age', 'education', 'studying', 'learning',
  'reading', 'raw.score', 'adjusted.score', 'region')

names(df.complete) <- df.newheader

dt.complete <- data.table(df.complete)
```

Past research indicated a lack of statistically significant result when comparing Sensing (high-detail) personality types and Intuitive (low-detail) personality types in nursing [@mbtiNursing], as well as an improvement for Intuitive (low-detail) personality types in engineering [@mbtiEngineering].

Still, the research team was surprised that the group means were the opposite of what was hypothesized given the specific task under evaluation. In particular, the team was surprised that providing additional instructions appeared to lower scores for Intuitive (low-detail) personality types.

```{r treatment_means_redux}
dt.complete[
  order(energy, in.treatment),
  .(mean = mean(adjusted.score), sd = sd(adjusted.score)),
  by = .(energy, in.treatment)
]
```

Given the lack of statistical power, this may have been a case of regression to the mean. Additional follow up will be needed in order to evaluate the model.

## Going on a Fishing Expedition

### Education as a Predictor

In performing covariate checks, the team observed that there was a statistically significant correlation between the outcome variable and education.

For an individual $i$, if we let $education_{i1}$ indicate if the individual finished education after completing high school, $education_{i2}$ indicate if the individual finished their education after receiving an associate's degree, $education_{i3}$ indicate if the individual finished their education after 3+ years without receiving a degree, $education_{i4}$ indicate if the individual received a bachelor's degree, and $education_{i5}$ indicate if the individual finsihed their education after receiving a master's degree or higher.

The following model adds education as a variable:

$$
\begin{split}
AdjustedScore_i = & \beta_0 +
  \beta_1 Sensing_i +
  \beta_2 InTreatment_i +
  \beta_3 InTreatment_i \times Sensing_i + \\ &
  \beta_4 Delaware_i +
  \beta_5 SouthCarolina_i + \\ &
  \beta_9 Education_{1_i} + \beta_{10} Education_{2_i} + \beta_{11} Education_{3_i} + \beta_{12} Education_{4_i} + \beta_{13} Education_{5_i}
\end{split}
$$

For a subject $i$, $Education_{1_i}$ indicates whether the highest level of education attained is a high school diploma or GED, $Education_{2_i}$ indicates whether the highest level of education attained is an associate's degree, $Education_{3_i}$ indicates whether the highest level of education attained is 3 or more years of college without a degree, $Education_{4_i}$ indicates whether the highest level of education attained is a bachelor's degree, and $Education_{5_i}$ indicates whether the highest level of education attained is a master's degree or higher.

This model with clustered standard errors is compared to the region-specific model using `stargazer` [@stargazer]:

\newpage

```{r fishing_expedition, results = 'asis'}
model.4 <- lm(
  adjusted.score ~ energy * in.treatment + region,
  data = df.complete)

# Use clustered standard errors

model.4.se <- sqrt(diag(cluster.vcov(model.4, ~ region)))

# Compute the education terms

df.complete$education1 = df.complete$education == 'High school diploma or GED'
df.complete$education2 = df.complete$education == "Associate's degree"
df.complete$education3 = df.complete$education == '3 or more years of college, no Bachelor’s or Master’s degree'
df.complete$education4 = df.complete$education == 'Bachelor’s degree'
df.complete$education5 = df.complete$education == 'Master’s degree or higher'

model.6 <- lm(
  adjusted.score ~ education1 + education2 + education3 + education4 + education5,
  data = df.complete)

model.7 <- lm(
  adjusted.score ~ energy * in.treatment + education1 + education2 + education3 + education4 + education5 + region,
  data = df.complete)

# Use clustered standard errors

model.7.se <- sqrt(diag(cluster.vcov(model.7, ~ region)))

stargazer(
  model.4, model.6, model.7,
  se = list(model.4.se, NULL, model.7.se),
  header = FALSE,
  type = stargazer.type,
  title = 'Fishing Expedition',
  dep.var.labels = c('Adjusted Score')
)
```

The addition of the education indicator variables increases the variance for the $InTreatment_i$ term and reduces the variance in the $Sensing_i$ and $InTreatment_i$ interaction term.

Note that the $R^2$ is substantially higher for education alone than for the region-specific model.

### True vs. False Questions

During the pilot study, the research team identified several questions on the questionnaire that were answered in the same way by all pilot study participants, suggesting that these questions were either too easy (in the case where all participants chose the correct answer) or too difficult (in the case where all participants chose the incorrect answer). Such questions have no power to discriminate between individuals who do recall details from a reading and individuals who do not recall details from a reading. As a result, the team decided to replace these questions for the actual experiment.

After identifying these questions that were both too easy and too difficult, the research team asked another question: will all questions be equally affected by the treatment primer? In other words, is it possible that the treatment will have a small effect on one question but a large effect on another question?

In this experiment, questions where the correct answer was FALSE included some elements of truth. As a result, false questions would be trickier than true questions, because if a participant were able to recall some of the details but not all of the details, the participant would be misled into making an incorrect guess.

Knowing this, then at a coarse-grained level, the research question could be operationalized as follows: in making the questions where the correct answer was FALSE trickier than the questions where the correct answer was TRUE, will the treatment primer have a different effect on these two classes of questions?

More simply, does the treatment have a different effect on true questions vs. false questions? The following linear regression model attempts to detect such an effect:

$$
\begin{split}
Proportion_j = & \beta_0 +
  \beta_1 CorrectAnswer_j +
  \beta_2 InTreatment_j +
  \beta_3 InTreatment_j \times CorrectAnswer_j
\end{split}
$$

For a given question $j$, $Proportion_j$ indicates the proportion of TRUE responses received for the question, $CorrectAnswer_j$ indicates whether the correct answer for the question is TRUE, and $InTreatment_j$ indicates whether the statistic corresponds to the average of the control group or the treatment group for that question.

```{r true_vs_false, results = 'asis'}
# Treatment or Control

df.assignment <- read.csv('data/Treatment or Control.csv') %>%
  select(c(-Timestamp)) %>% filter(Subject.ID >= 100)

# Reading Recall Questionnaire

df.recall.raw <- read.csv('data/Reading Recall Questionnaire.csv') %>%
  select(c(-Timestamp,-X)) %>% filter(Subject.ID >= 100)

# Reading Recall Questionnaire answer key

get.recall.responses <- function(x) {
  list(
    Subject.ID = t(x)[1],
    Responses = as.logical(t(x)[-1])
  )
}

get.recall.responses.vector <- function(x) {
  c(as.numeric(t(x)[1]), as.logical(t(x)[-1]))
}

df.recall.answer.key.raw <- read.csv('data/Reading Recall Questionnaire.csv') %>%
  select(c(-Timestamp,-X)) %>% filter(Subject.ID == 0)

df.recall.answer.key <- get.recall.responses(df.recall.answer.key.raw[1, ])

# Reading Recall Questionnaire scoring

dt.recall.responses.vector <- data.table(
  t(apply(df.recall.raw, 1, get.recall.responses.vector))
)

colnames(dt.recall.responses.vector) <- c('Subject.ID', sprintf('Question.%02d', 1:40))

dt.recall.responses <-merge(dt.recall.responses.vector, df.assignment, by = 'Subject.ID')
dt.recall.responses[, Subject.ID := NULL]
dt.recall.responses[, Experimenter := NULL]

# Proportions

mean.without.na <- function(x) {
  mean(x, na.rm = TRUE)
}

dt.question.ratio <- dt.recall.responses[,
  lapply(.SD, mean.without.na), by = Treatment.vs..Control.
]

dt.answer.ratio <- dcast.data.table(
  melt(dt.question.ratio, id.vars = 'Treatment.vs..Control.'),
  variable ~ Treatment.vs..Control.
)

setnames(dt.answer.ratio, c('Question', 'Control', 'Treatment'))

dt.answer.ratio[,
  Correct.Answer := df.recall.answer.key$Responses
]

dt.answer.ratio.melted <- melt(
  dt.answer.ratio,
  id.vars = c('Question', 'Correct.Answer')
)

model.8 <- lm(
  value ~ Correct.Answer * variable,
  data = dt.answer.ratio.melted
)

stargazer(
  model.8,
  header = FALSE,
  type = stargazer.type,
  title = 'True vs. False Answers',
  dep.var.labels = c('Proportion of TRUE Responses'),
  covariate.labels = c(
    'Correct Answer is TRUE',
    'Treatment Primer',
    'Correct Answer is TRUE x Treatment Primer')
)
```

According to this model, $\hat{\beta}_2 = 0.010$, indicating that for questions where the correct answer is FALSE, the treatment increased the proportion of TRUE responses by 1.0%, thus increasing the number of incorrect responses. $\hat{\beta}_2 + \hat{\beta}_3 = 0.059$, indicating that for questions where the correct answer is TRUE, the treatment increased the proportion of TRUE responses by 5.9%.

```{r true_vs_false_model, fig.height=3, fig.width=8}
ggplot(dt.answer.ratio.melted, aes(x = value, colour = variable)) +
  ggtitle('Proportion of TRUE Responses') +
  geom_density() + facet_grid(~ Correct.Answer) +
  theme_bw() + theme(axis.text.x=element_blank(), axis.title.x=element_blank(), legend.title=element_blank())
```

This difference is made more apparent when visualized as density plots, as the difference between the control group and the treatment group when the correct answer is TRUE is much more visible than the difference between the control group and the treatment group when the correct answer is FALSE.
